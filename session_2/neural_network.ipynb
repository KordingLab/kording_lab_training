{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Neural Network for Classification problem\n",
    "\n",
    "**Reference** Andrew Ng, Machine Learning, Coursera notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Definitions\n",
    "\n",
    "![alt text](neural_nets.png)\n",
    "\n",
    "Suppose we have training set \n",
    "$$\\{(\\mathbf{x}^{(1)}, \\mathbf{y}^{(1)}), (\\mathbf{x}^{(2)}, \\mathbf{y}^{(2)}), \\ldots, (\\mathbf{x}^{(m)}, \\mathbf{y}^{(m)}) \\}$$\n",
    "where $L = \\text{total no. of layers in network}$, $s_l = \\text{no. of units in layer }l$ and $m$ = length of training set\n",
    "\n",
    "that is, $s_1$ is number of units in layer 1 (example below is 3)\n",
    "\n",
    "$j$ defined as node $j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple one layer model\n",
    "\n",
    "![alt text](neural_nets_simple.png)\n",
    "\n",
    "$$y = \\cfrac{1}{1 + \\exp(\\theta_1 x_1 + \\theta_2 x_2 + \\theta_3 x_3)} =  \\cfrac{1}{1 + \\exp(\\mathbf{\\theta}^\\top \\mathbf{x})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output for classification\n",
    "\n",
    "For binary class\n",
    "\n",
    "$$y = 0 \\text{ or } 1$$\n",
    "\n",
    "For Multiclass classification\n",
    "\n",
    "$$\\mathbf{y} = \\begin{bmatrix} 1\\\\0\\\\ \\ldots \\\\0 \\end{bmatrix}, \\begin{bmatrix} 0 \\\\ 1 \\\\ \\ldots \\\\0 \\end{bmatrix}, \\ldots$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Cost function\n",
    "\n",
    "**Logistic Regression with regularization**\n",
    "\n",
    "$$J(\\theta) = -\\frac{1}{m} \\left[ \\sum_{i=1}^m y^{(i)} \\log h_{\\theta}(x^{(i)}) + (1 - y^{(i)}) \\log(1 - h_{\\theta}(x^{(i)}) )) \\right] + \\frac{\\lambda}{2m} \\sum_{j=1}^n \\theta_{j}^2 $$ \n",
    "\n",
    "**Neural network**\n",
    "\n",
    "$$J(\\theta) = -\\frac{1}{m} \\left[ \\sum_{i=1}^m \\sum_{k=1}^{K} y_{k}^{(i)} \\log h_{\\theta}(x^{(i)})_k + (1 - y_{k}^{(i)}) \\log(1 - h_{\\theta}(x^{(i)})_{k} )) \\right] + \\frac{\\lambda}{2m} \\sum_{l=1}^{L-1} \\sum_{i=1}^{s_l} \\sum_{j=1}^{s_l + 1} (\\Theta_{ji}^{(l)})^2 $$ \n",
    "\n",
    "*note* that we exclude $\\Theta_{i0}$ when we compute regularization terms\n",
    "\n",
    "We want to minimize cost function $$\\min_{\\theta} J(\\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of 4 layers\n",
    "\n",
    "**Forward propagation**\n",
    "\n",
    "\\begin{align}\n",
    "a^{(1)} &= x\\\\\n",
    "z^{(2)} &= \\Theta^{(1)}a^{(1)}\\\\\n",
    "a^{(2)} &= g(z^{2}) \\text{ (add $a_0^{(2)}$)}\\\\\n",
    "z^{(3)} &= \\Theta^{(2)} a^{(2)}\\\\\n",
    "a^{(3)} &= g(z^{(3)})  \\text{ (add $a_0^{(3)}$)}\\\\\n",
    "z^{(4)} &= \\Theta^{(3)} a^{(3)}\\\\\n",
    "a^{(4)} &= h_{\\Theta}(x) = g(z^{(4)})\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\delta_j^{(l)}$ is error of node $j$ in layer $l$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Back propagation**\n",
    "\\begin{align}\n",
    "\\delta^{(4)} &= a^{(4)} - y\\\\\n",
    "\\delta^{(3)} &= (\\Theta^{(3)})^\\top \\delta^{(4)}.* g'(z^{(3)}) \\\\\n",
    "\\delta^{(2)} &= (\\Theta^{(2)})^\\top \\delta^{(3)}.* g'(z^{(2)})\n",
    "\\end{align}\n",
    "\n",
    "where we can compute $g'(z^{(3)}) = a^{(3)}.*(1 - a^{(3)})$, $g'(z^{(2)}) = a^{(2)}.*(1 - a^{(2)})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of X and y:  (5000, 400) (5000, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "mnist = loadmat('mnist_data.mat')\n",
    "X = mnist['X']\n",
    "y = mnist['y']\n",
    "print \"Size of X and y: \", X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    g = 1.0/(1.0 + np.exp(-z))\n",
    "    return g\n",
    "\n",
    "def sigmoid_grad(z):\n",
    "    g = sigmoid(z)*(1 - sigmoid(z))\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back propagation algorithm\n",
    "\n",
    "$\\Delta_{ij}^{(l)} = 0$ for all $l, i, j$\n",
    "\n",
    "for $i = 1 \\text{ to } m:$ (every training examples)\n",
    "\n",
    "- set $a^{(1)} = x^{(i)}$\n",
    "- perform forward propagation to compute $a^{(l)}$\n",
    "- compute $\\delta^{(L)} = a^{(L)} - y^{(i)}$\n",
    "- compute $\\delta^{(L-1)}, \\delta^{(L-2)}, \\ldots, \\delta^{(2)}$ (no $\\delta^{(1)}$ because it's input layer)\n",
    "- $\\Delta_{ij}^{(l)} = \\Delta_{ij}^{(l)} + a_j^{(l)} \\delta_i^{(l+1)}$ or $\\Delta^{(l)} = \\Delta^{(l)} + \\delta^{(l+1)} (a^{(l)})^\\top$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "$D_{ij}^{(l)} = \\frac{1}{m} \\Delta_{ij}^{(l)} + \\lambda \\Theta_{ij}^{(l)}$\n",
    "\n",
    "$D_{ij}^{(l)} = \\frac{1}{m} \\Delta_{ij}^{(l)}$\n",
    "\n",
    "where we can show that $\\frac{\\partial}{\\partial \\Theta_{ij}^{(l)}} J(\\Theta) = D_{ij}^{(l)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**\n",
    "\n",
    "We use sample MNIST data which each has 20 $\\times$ 20 pixels. Therefore, we need 400 input layer units. We use only 3 layers which input then map to 25 units in second layers. The output has 10 classes (number from 0 to 9) which means 10 units in third layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 401) (10, 26)\n"
     ]
    }
   ],
   "source": [
    "W = loadmat('weights.mat')\n",
    "Theta1 = W['Theta1']\n",
    "Theta2 = W['Theta2']\n",
    "print Theta1.shape, Theta2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialize neural network parameters**\n",
    "\n",
    "One way to initialize $\\Theta^{(i)}$ is to generate uniformly distributed values between $[ -\\epsilon_{init}, \\epsilon_{init}]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_weight(L_in, L_out):\n",
    "    \"\"\"Randomly initialize the weights of a layer\"\"\"\n",
    "    epsilon_init = 0.12\n",
    "    W = np.random.rand(L_out, 1+L_in)*2*epsilon_init - epsilon_init\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost_grad(Theta1, Theta2, X, y, lm=1.0):\n",
    "    \"\"\"\n",
    "    Compute cost and gradient of NN parameters for one iteration\n",
    "    \"\"\"\n",
    "    # initialize few parameters\n",
    "    n, m = X.shape\n",
    "    J = 0\n",
    "    Theta1_grad = np.zeros(Theta1.shape)\n",
    "    Theta2_grad = np.zeros(Theta2.shape)\n",
    "    D1 = np.zeros(Theta1.shape)\n",
    "    D2 = np.zeros(Theta2.shape)\n",
    "    \n",
    "    # forward propagation\n",
    "    a1 = np.concatenate((np.ones((n,1)), X), axis=1)\n",
    "    z2 = a1.dot(Theta1.T)\n",
    "    a2 = sigmoid(z2)\n",
    "    a2 = np.concatenate((np.ones((n,1)), a2), axis=1)\n",
    "    z3 = a2.dot(Theta2.T)\n",
    "    a3 = sigmoid(z3)\n",
    "    h = a3\n",
    "    \n",
    "    # tranform y to Y, sparse format\n",
    "    Y = np.zeros(h.shape)\n",
    "    for i in range(n):\n",
    "        Y[i, y[i]-1] = 1\n",
    "    \n",
    "    # compute cost\n",
    "    J = -(1.0/n)*np.sum(np.sum(Y*np.log(h) + (1 - Y)*np.log(1 - h))) + \\\n",
    "         (lm/(2.0*n))*(np.sum(np.sum(Theta1[:,1::]**2)) + np.sum(np.sum(Theta2[:,1::]**2)))\n",
    "    \n",
    "    # back propagation\n",
    "    delta3 = a3 - Y\n",
    "    delta2 = delta3.dot(Theta2[:, 1::])*sigmoid_grad(z2)\n",
    "    \n",
    "    D1 = D1 + (delta2.T).dot(a1)\n",
    "    Theta1_grad[:,0] = (1.0/m)*D1[:,0]\n",
    "    Theta1_grad[:,1::] = (1.0/m)*D1[:,1::] + (lm/m)*Theta1[:,1::]\n",
    "    \n",
    "    D2 = D2 + (delta3.T).dot(a2)\n",
    "    Theta2_grad[:,0] = (1.0/m)*D2[:,0]\n",
    "    Theta2_grad[:,1::] = (1.0/m)*D2[:,1::] + (lm/m)*Theta2[:,1::]\n",
    "    \n",
    "    return J, Theta1_grad, Theta2_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cost function given Theta1 and Theta2 =  0.383769859091\n"
     ]
    }
   ],
   "source": [
    "J, Theta1_grad, Theta2_grad = compute_cost_grad(Theta1, Theta2, X, y)\n",
    "print 'The cost function given Theta1 and Theta2 = ', J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(Theta1, Theta2, X):\n",
    "    n, m = X.shape\n",
    "    h1 = sigmoid(np.concatenate((np.ones((n,1)), X), axis=1).dot(Theta1.T))\n",
    "    h2 = sigmoid(np.concatenate((np.ones((n,1)), h1), axis=1).dot(Theta2.T))\n",
    "    p = np.atleast_2d(np.argmax(h2, axis=1) + 1).T\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work flow for computing final $\\Theta_1, \\Theta_2$\n",
    "\n",
    "To actually minimize the cost function, typically use `fmincon` or `fmincg` to pass cost and gradient to the function i.e.\n",
    "\n",
    "`[nn_params, cost] = fminunc(compute_cost, initial_nn_params);`\n",
    "\n",
    "However, we can try simple gradient descent to see the convolution of cost over iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use simple gradient descent algorithm to get final Theta1 and Theta2\n",
    "n_iter = 1000\n",
    "epsilon = 0.3\n",
    "Theta1 = init_weight(400, 25)\n",
    "Theta2 = init_weight(25, 10)\n",
    "J_all = []\n",
    "\n",
    "for i in range(n_iter):\n",
    "    J, Theta1_grad, Theta2_grad = compute_cost_grad(Theta1, Theta2, X, y)\n",
    "    Theta1 = (Theta1 - epsilon*Theta1_grad)\n",
    "    Theta2 = (Theta2 - epsilon*Theta2_grad)\n",
    "    J_all.append(J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy =  95.86\n"
     ]
    }
   ],
   "source": [
    "# using Theta1, Theta2 from gradient descent\n",
    "y_pred = predict(Theta1, Theta2, X)\n",
    "print 'Training accuracy = ', np.mean(y == y_pred)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy =  97.52\n"
     ]
    }
   ],
   "source": [
    "# converged Theta1, Theta2 from fmincg\n",
    "W = loadmat('weights.mat')\n",
    "Theta1 = W['Theta1']\n",
    "Theta2 = W['Theta2']\n",
    "y_pred = predict(Theta1, Theta2, X)\n",
    "print 'Training accuracy = ', np.mean(y == y_pred)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEZCAYAAAB8culNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmnVWKgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1079e2dd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(n_iter), J_all)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Cost (J)')\n",
    "plt.title('Cost function over iteration')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
