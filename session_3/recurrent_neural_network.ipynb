{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **Recurrent Neural Network (RNN) - Introduction**\n",
    "\n",
    "**All sources are from:**\n",
    "- [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "- [The Unreasonable Effectiveness of Recurrent Neural Networks by Andrej Karpathy](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "- [Supervised Sequence Labelling with Recurrent Neural Networks by Alex Graves](http://www.cs.toronto.edu/~graves/preprint.pdf)\n",
    "- [RECURRENT NEURAL NETWORKS TUTORIAL](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/)\n",
    "\n",
    "**Covers:**\n",
    "- Structure of RNN\n",
    "- Structure of LSTM\n",
    "- Tips and tricks\n",
    "- Few more examples\n",
    "- Coding: Python example (character prediction in RNN by Karpathy), TensorFlow example\n",
    "\n",
    "**Notation: ** notation is a bit messy in the tutorial. Every figures has different notations.\n",
    "\n",
    "### **Structure of RNN**\n",
    "\n",
    "Here, Karpathy descibes RNN using input/output relations\n",
    "\n",
    "<img width=\"800\" src=\"http://karpathy.github.io/assets/rnn/diags.jpeg\"/>\n",
    "**(1)** vanilla mode (fixed size input) **(2)** Sequence output (e.g. image captioning) **(3)** Sequence input (sentiment analysis) **(4)** Sequence input and sequence output (e.g. machine translation)\n",
    "\n",
    "\n",
    "**Example of character prediction**\n",
    "<img width=\"400\" src=\"http://karpathy.github.io/assets/rnn/charseq.jpeg\"/>\n",
    "\n",
    "\n",
    "**Recurrent Neural Networks**\n",
    "\n",
    "<img width=\"100\" src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-rolled.png\"/>\n",
    "\n",
    "- RNNs makes use of sequential information\n",
    "- Another way to think about RNNs is that they have a “memory” which captures information (through $s_t$)\n",
    "\n",
    "\n",
    "**Unrolled Recurrent Neural Networks**\n",
    "\n",
    "<img width=\"500\" src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png\"/>\n",
    "\n",
    "\n",
    "input $x_i$ with output $h_i$ with chunk of neural netwok $A$\n",
    "\n",
    "\n",
    "### **Parameters**\n",
    "\n",
    "We only have $U, W, V$ ($W_1, W_2, W_3$) only, and it's the same in every time steps\n",
    "<img width=\"500\" src=\"unfold_rnn.png\"/>\n",
    "\n",
    "Standard RNN equations are as follows (sorry the notation from the figure is not the same as what we write):\n",
    "\n",
    "<img width=\"800\" src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-SimpleRNN.png\"/>\n",
    "\n",
    "$$s_t = \\tanh(U x_{t} + W s_{t-1})$$ \n",
    "$$\\hat{y}_t = \\text{softmax}(V s_{t}) $$\n",
    "$$E(y, \\hat{y}) = -\\sum_{t}  y_t \\log \\hat{y}_t$$\n",
    "\n",
    "where\n",
    "- $x_t$ is the input at time step $t$\n",
    "- $s_t$ is the hidden state at time step $t$ (memory)\n",
    "- $\\hat{y}_t$ is the output at step $t$\n",
    "- $E(y, \\hat{y})$ is cross-entropy loss function  (for all traning examples)\n",
    "\n",
    "**note that** \n",
    "- we can re-write equation as $s_t = \\tanh([U, W] \\cdot [x_t, s_{t-1}])$\n",
    "- RNN shares the same parameters across all steps\n",
    "\n",
    "**Pseudo-code for RNN - Forward propagation**\n",
    "\n",
    "```python\n",
    "class RNN:\n",
    "    def step(self, x):\n",
    "        self.s = np.tanh(np.dot(self.W, self.s) + np.dot(self.U, x))\n",
    "        y_hat = np.dot(self.V, self.s)\n",
    "        return y_hat\n",
    "```\n",
    "\n",
    "**Different type of activation function**\n",
    "\n",
    "Last lecture, we learned only sigmoid. Here is the hyperbolic function that people also use.\n",
    "\n",
    "$$\\tanh(x) = \\cfrac{e^{2x} - 1}{ e^{2x} + 1}$$\n",
    "$$\\sigma(x) = \\cfrac{1}{1 + e^{-x}} $$\n",
    "$$\\tanh(x) = 2 \\sigma(2x) - 1 $$\n",
    "\n",
    "**Back propagation**\n",
    "\n",
    "- Similar to traditional neural network. Since parameters are shared across time step, gradient then depends not only on current time step but **previous time steps**\n",
    "- We need to backpropagate (e.g. t=4, we have to backpropagate 3 steps) and sum up the gradient\n",
    "- **note** BPTT have difficulties learning long-term dependencies (gradient vanishing) > can be solved by certain types of RNN like LSTMs\n",
    "- We'll talk more later on... now we just do forward propagation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If x = 1.0 Output from sigmiod and tanh is 0.761594155956, 0.761594155956 respectively\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_grad(z):\n",
    "    g = sigmoid(z)*(1 - sigmoid(z))\n",
    "    return g\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_grad(z):\n",
    "    g = 1 - (np.tanh(z)**2)\n",
    "    return g\n",
    "\n",
    "x = 1.0\n",
    "print 'If x = %s Output from sigmiod and tanh is %s, %s respectively' % (x, 2*sigmoid(2*x)-1, tanh(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Extension\n",
    "\n",
    "\n",
    "- **Bidirectional RNNs** output at time $t$ depend on the previous elements in the sequence and future elements (e.g predicting missing words)\n",
    "<img width=\"400\" src=\"http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/09/bidirectional-rnn.png\"/>\n",
    "\n",
    "- **Deep (Bidirectional) RNNs**\n",
    "<img width=\"400\" src=\"http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/09/Screen-Shot-2015-09-16-at-2.21.51-PM.png\"/>\n",
    "\n",
    "- **LSTM network** LSTMs don’t have a fundamentally different architecture from RNNs, but they use a different function to compute the hidden state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Back-Propagation through time**\n",
    "\n",
    "**Back-propagation through time (BPTT)**\n",
    "\n",
    "what we have before are forward propagation equations:\n",
    "\n",
    "$$s_t = \\tanh(U x_{t} + W s_{t-1})$$ \n",
    "$$\\hat{y}_t = \\text{softmax}(V s_{t}) $$\n",
    "\n",
    "and loss by cross entropy loss, \n",
    "$$E_t(y_t, \\hat{y}_t) = - y_t \\log \\hat{y}_t$$\n",
    "$$E = \\sum_{t} E_t(y_t, \\hat{y}_t) = -\\sum_{t}  y_t \\log \\hat{y}_t$$\n",
    "\n",
    "where $y_t$ is correct output, $\\hat{y}_t$ is our prediction.\n",
    "\n",
    "\n",
    "<img width=\"400\" src=\"http://www.wildml.com/wp-content/uploads/2015/10/rnn-bptt1.png\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal: ** calculate the gradients of the error with respect to our parameters $U, V, W$\n",
    "\n",
    "We'll use $E_3$ as an example (example from blog post)\n",
    "\n",
    "$$\\cfrac{\\partial E_3}{\\partial V} = \\cfrac{\\partial E_3}{\\partial \\hat{y}_3} \\cfrac{\\partial \\hat{y}_3}{\\partial V} = \\cfrac{\\partial E_3}{\\partial \\hat{y}_3} \\cfrac{\\partial \\hat{y}_3}{\\partial z_3} \\cfrac{\\partial z_3}{\\partial V} = (\\hat{y}_3 - y_3) s_3 $$\n",
    "\n",
    "where $z_3 = V s_3$. We can see that $\\cfrac{\\partial E_3}{\\partial V}$ only depends on the values at the current time step\n",
    "\n",
    "\n",
    "However, for $\\cfrac{\\partial E_3}{\\partial W}$, story is different\n",
    "\n",
    "$$\\cfrac{\\partial E_3}{\\partial W} = \\cfrac{\\partial E_3}{\\partial \\hat{y}_3} \\cfrac{\\partial \\hat{y}_3}{\\partial s_3} \\cfrac{\\partial s_3}{\\partial W} $$\n",
    "\n",
    "Now, $s_3 = \\tanh(U x_3 + W s_2)$ which depends on $s_2$ (and yes, $s_1$ also)\n",
    "\n",
    "$$\\cfrac{\\partial E_3}{\\partial V} = \\sum_{k=0}^3 \\cfrac{\\partial E_3}{\\partial \\hat{y}_3} \\cfrac{\\partial \\hat{y}_3}{\\partial s_3} \\cfrac{\\partial s_3}{\\partial s_k} \\cfrac{\\partial s_k}{\\partial W}  $$\n",
    "\n",
    "\n",
    "<img width=\"400\" src=\"http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/10/rnn-bptt-with-gradients.png\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Long short-term memory networks (LSTM)**\n",
    "\n",
    "LSTM are a special kind of RNN, capable of learning long-term dependencies.\n",
    "\n",
    "\n",
    "<img width=\"800\" src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png\"/>\n",
    "\n",
    "**Forget gate layer** \n",
    "<img width=\"800\" src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-f.png\"/>\n",
    "\n",
    "**Input gate layer** decides which values we’ll update \n",
    "<img width=\"800\" src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-i.png\"/>\n",
    "\n",
    "**The new candidate values**\n",
    "<img width=\"800\" src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-C.png\"/>\n",
    "\n",
    "**Output gate**Decide what we’re going to output\n",
    "<img width=\"800\" src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-o.png\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Additional tricks for RNN training**\n",
    "\n",
    "**Tips** here are some tips and tricks from Karpathy in RNN\n",
    "\n",
    "- **RMSProp/Adam/Adagrad** (SGD has high sensitivity)\n",
    "- **Clip gradient** 5.0 is a common value to use, suggested by Mikolov. Prevent exploding gradient problem (see also vanishing gradient)\n",
    "- **Initialize forget gates** with high bias to encourage remembering at start\n",
    "- **Regularization** L2 regularization is not very common. Dropout always good along depth, *NOT* along time\n",
    "\n",
    "<img width=\"400\" src=\"vanishing.png\"/>\n",
    "\n",
    "### **More example**\n",
    "\n",
    "**Image captioning**\n",
    "\n",
    "<img width=\"400\" src=\"image_captioning.png\"/>\n",
    "\n",
    "\n",
    "The RNN is conditioned on the image information at the first time step. START and END are special tokens. [See full paper here](http://cs.stanford.edu/people/karpathy/cvpr2015.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## **Coding session**\n",
    "\n",
    "we will follow example from https://github.com/dennybritz/rnn-tutorial-rnnlm which has blog post [here](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-rnn-with-python-numpy-and-theano/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
