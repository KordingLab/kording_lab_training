{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Recurrent Neural Network (RNN)\n",
    "\n",
    "All sources are from:\n",
    "- [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "- [The Unreasonable Effectiveness of Recurrent Neural Networks by Andrej Karpathy](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "- [Supervised Sequence Labelling with Recurrent Neural Networks by Alex Graves](http://www.cs.toronto.edu/~graves/preprint.pdf)\n",
    "\n",
    "### Structure of RNN\n",
    "\n",
    "Here, Karpathy descibes RNN using input/output relations\n",
    "\n",
    "\n",
    "<figure>\n",
    "    <center>\n",
    "        <img width=\"800\" src=\"http://karpathy.github.io/assets/rnn/diags.jpeg\"/>\n",
    "        **(1)** vanilla mode (fixed size input) **(2)** Sequence output (e.g. image captioning) **(3)** Sequence input (sentiment analysis) **(4)** Sequence input and sequence output (e.g. machine translation)\n",
    "        \n",
    "        <img width=\"800\" src=\"http://karpathy.github.io/assets/rnn/charseq.jpeg\"/>\n",
    "        Example of character prediction\n",
    "        \n",
    "        <img width=\"100\" src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-rolled.png\"/>\n",
    "        Recurrent Neural Networks\n",
    "        <img width=\"500\" src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png\"/>\n",
    "        Unrolled recurrent neural network\n",
    "    </center>\n",
    "</figure>\n",
    "\n",
    "input $x_i$ with output $h_i$ with chunk of neural netwok $A$\n",
    "\n",
    "\n",
    "### Parameters\n",
    "\n",
    "<img width=\"500\" src=\"unfold_rnn.png\"/>\n",
    "\n",
    "Standard RNN equations are as follows:\n",
    "\n",
    "<img width=\"800\" src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-SimpleRNN.png\"/>\n",
    "\n",
    "$$h_t = \\sigma(W^{(hh)} h_{t-1} + W^{(hx)} x_{t})$$ \n",
    "$$\\hat{y}_t = \\text{softmax}(W^{(s)} h_{t}) $$\n",
    "$$J = -\\frac{1}{T} \\sum_{t=1}^{T} \\sum_{j=1}^{|V|} y_{t,j} \\log \\hat{y}_{t, j}$$\n",
    "\n",
    "\n",
    "note that we can re-write equation as $h_t = \\sigma(W [h_{t-1}, x_t])$ also\n",
    "\n",
    "**Forward propagation**\n",
    "\n",
    "Last lecture, we learned only sigmoid. Here is the hyperbolic function that people also use.\n",
    "\n",
    "$$\\tanh(x) = \\cfrac{e^{2x} - 1}{ e^{2x} + 1}$$\n",
    "$$\\sigma(x) = \\cfrac{1}{1 + e^{-x}} $$\n",
    "$$\\tanh(x) = 2 \\sigma(2x) - 1 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If x = 1.0 Output from sigmiod and tanh is 0.761594155956, 0.761594155956 respectively\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_grad(z):\n",
    "    g = sigmoid(z)*(1 - sigmoid(z))\n",
    "    return g\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_grad(z):\n",
    "    g = 1 - (np.tanh(z)**2)\n",
    "    return g\n",
    "\n",
    "x = 1.0\n",
    "print 'If x = %s Output from sigmiod and tanh is %s, %s respectively' % (x, 2*sigmoid(2*x)-1, tanh(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long short-term memory networks (LSTM)\n",
    "\n",
    "LSTM are a special kind of RNN, capable of learning long-term dependencies.\n",
    "\n",
    "\n",
    "<img width=\"800\" src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png\"/>\n",
    "\n",
    "**Forget gate layer** \n",
    "<img width=\"800\" src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-f.png\"/>\n",
    "\n",
    "**Input gate layer** decides which values we’ll update \n",
    "<img width=\"800\" src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-i.png\"/>\n",
    "\n",
    "**The new candidate values**\n",
    "<img width=\"800\" src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-C.png\"/>\n",
    "\n",
    "**Output gate**Decide what we’re going to output\n",
    "<img width=\"800\" src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-o.png\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional tricks for RNN training\n",
    "\n",
    "**Tips** here are some tips and tricks from Karpathy in RNN\n",
    "\n",
    "- **RMSProp/Adam/Adagrad** (SGD has high sensitivity)\n",
    "- **Clip gradient** 5.0 is a common value to use, suggested by Mikolov. Prevent exploding gradient problem (see also vanishing gradient)\n",
    "- **Initialize forget gates** with high bias to encourage remembering at start\n",
    "- **Regularization** L2 regularization is not very common. Dropout always good along depth, *NOT* along time\n",
    "\n",
    "<img width=\"400\" src=\"vanishing.png\"/>\n",
    "\n",
    "### More example\n",
    "\n",
    "**Image captioning**\n",
    "\n",
    "<img width=\"400\" src=\"image_captioning.png\"/>\n",
    "\n",
    "\n",
    "The RNN is conditioned on the image information at the first time step. START and END are special tokens. [See full paper here](http://cs.stanford.edu/people/karpathy/cvpr2015.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
